# **Operational Manual & Decision Log (OMDL)**

  * **Version:** 10.0
  * **Date:** July 28, 2025
  * **Status:** Final - Post-Migration Baseline

## 1.0 Document Purpose

This document codifies the roles, workflow, development protocols, and decision-making history for the Project Arrowhead project. It serves as the definitive record of lessons learned and operational wisdom.

-----

## 2.0 Core Principles

  * A Failed Implementation is a Data Point About a Flawed Prompt.
  * Automated Verification is the Foundation. A feature is not "done" until its corresponding automated test passes.
  * The Architect's Job is to Build Guardrails, Not Drive the Car.
  * A test that doesn't simulate a real user action is a vanity test.

-----

## 3.0 Core Development & Diagnostic Protocols

  * **The Protocol of Grounding (Read-Before-Write):** Before any file modification, the assistant must first read the exact, current content of the code.
  * **The "Three Strikes" Protocol:** If a specific technical approach fails three consecutive times, declare it "unviable" and pivot to a fundamentally different strategy.
  * **The "Circuit Breaker Protocol":** If an AI instance fails on the same task twice, the Architect must intervene, halt the process, and issue a new, simplified prompt.
  * **The Sentry Protocol (Test-First Debugging):** When a bug is found during manual testing, it must be addressed with a strict five-step process: 1) REPORT the bug, 2) REPLICATE the bug by writing a new, failing automated test, 3) REMEDIATE the application code, 4) RE-RUN the new test to prove it passes, and 5) REGRESS by running the full test suite.
  * **The Parity Protocol (Test-Driven Migration):** During an architectural migration, the existing, working E2E test suite for the legacy application serves as the definitive, machine-readable specification for the new application. True feature parity is only achieved when a corresponding test for every feature passes in the new architecture.
  * **The Digital Twin Protocol (Meticulous Audit):** Before a component is remediated to achieve parity, a formal, element-by-element audit must be conducted, led by the Architect. The deliverable of this audit is a "Parity Audit Checklist" which must be signed off on by the Architect before remediation begins.
  * **The Human Handoff Protocol (Manual Testing):** At the conclusion of a successful development sprint, the AI's final output will be a clear set of instructions for the human stakeholder to manually start the server and prepare their browser (e.g., by disabling the cache) to begin testing.
  * **The Sequential Module Navigation Protocol:** Module completion must follow the original user journey flow: Brainstorm → Choose → Objectives → Dashboard. Any deviation breaks user experience parity and must be corrected via context-aware navigation logic.

-----

## 4.0 Key Operational Models

  * **The "Hands-On Architecture" Model:** The Lead Architect does not passively approve plans generated by the AI. The Architect actively leads all analysis, using the AI as a high-speed diagnostic tool to fetch raw data (code, logs, etc.). The Architect is responsible for performing the analysis, forming the diagnosis, and creating the subsequent action plan.

-----

## 5.0 Key Decision Log
*(Most recent decisions at the top)*

**Decision (July 28, 2025):** Successfully completed Operation: True Parity migration achieving 100% functional parity across all modules.  
**Rationale:** Systematic application of Parity Protocol with test-driven validation proved reliable for complex architectural migrations.

**Decision (July 28, 2025):** Implemented Sequential Module Navigation Protocol to resolve critical navigation discrepancies.  
**Rationale:** Navigation audit revealed user flow regressions that broke the seamless journey experience from the original application.

**Decision (July 28, 2025):** Adopted "Digital Twin" operational model for complex audits and diagnostics.  
**Rationale:** Architect-led analysis with AI as diagnostic tool proved more effective than AI-generated plans for complex technical investigations.

**Decision (July 18, 2025):** Added "Case Study #3: The E2E Test Suite Deadlock" to this document.  
**Rationale:** To permanently codify the lessons learned from a critical, multi-sprint failure in the testing environment.

**Decision (July 18, 2025):** Adopted the "Unified Export Strategy."  
**Rationale:** An application audit revealed inconsistent and missing export features. This strategy creates a consistent user experience and provides a clear blueprint for development.

**Decision (July 2025):** Permanently decoupled the test runner from the application server.  
**Rationale:** A series of cascading failures proved that having the test runner manage the server lifecycle via a spawn command was fundamentally unstable. The new, mandatory workflow requires starting the server manually in a separate process before running npm test.

**Decision (July 2025):** Adopted advanced AI interaction protocols ("Three Strikes," "Verification Tiers," "Circuit Breaker").  
**Rationale:** To prevent inefficient "debug-by-patching" loops and to create a more structured, hypothesis-driven methodology for development and debugging, especially when working with AI assistants.

**Decision (July 2025):** Migrated project from a cloud IDE to a local Python/Flask development environment.  
**Rationale:** To establish a more standard, professional, and stable development workflow that better supports automated testing and dependency management.

**Decision (July 2025):** Pivoted to "Thinking Tool" MVP.  
**Rationale:** The project's unique value is the guided thinking process, not competing with established project management tools. This decision focused the scope on the core user experience.

**Decision (July 2025):** Adopted a client-side architecture with no backend or user accounts.  
**Rationale:** Drastically reduces complexity, development time, and cost. Allows for rapid iteration on the core user experience using browser localStorage for session persistence.

-----

## 6.0 Appendix A: Key Failure Analyses & Lessons Learned

### Case Study #1: The Task List "Heisenbug"

  * **Synopsis:** During initial development, the Task List page exhibited erratic behavior. Tasks would sometimes fail to render after being added, or status changes would not appear on the UI, creating an inconsistent user experience that was difficult to reproduce reliably.
  * **Root Cause:** A compounded failure: 1) A CSS stacking context bug was causing the Bootstrap modal's backdrop to occasionally cover UI elements. 2) A race condition existed between the Bootstrap modal's hide animation and the function that re-rendered the task list. 3) The browser's HTML parser was silently failing on a malformed `<tbody>` element.
  * **Solution:** The rendering logic was completely refactored into a robust `renderTasks` function that rebuilds the entire task list from the data model. The malformed HTML was corrected. The event listener was updated to fire only after the Bootstrap modal's `hidden.bs.modal` event.
  * **Core Lessons:**
      * **API Skepticism:** Do not assume third-party libraries will have predictable timing; always use their provided event hooks.
      * **Defensive Rendering:** Rebuilding state from a data model is more reliable than surgically manipulating individual DOM elements.
      * **Visual Bugs Have Technical Roots:** A visual glitch is often a symptom of an underlying structural (HTML) or logical (JavaScript) error.

### Case Study #2: The Tooling & Execution Failures

  * **Synopsis:** A series of development sprints were plagued by the AI assistant's inability to reliably edit files using its built-in tools. `edit_file` commands would frequently fail, report success but make no changes, or corrupt the target file.
  * **Root Cause:** The AI's tool was highly sensitive to whitespace and context, making partial file edits fragile. This was compounded by the AI holding flawed assumptions about the project's file structure.
  * **Solution:** The workflow was updated to favor a "full file replacement" strategy over surgical edits for complex changes. The "Protocol of Grounding" was established, requiring the AI to list directory contents to verify file paths before acting.
  * **Core Lessons:**
      * **The Environment is a Variable:** Do not assume the AI's understanding of the file system is accurate; always verify.
      * **Tools Can Be Brittle:** When a tool proves unreliable, pivot to a more robust, heavy-handed approach.

### Case Study #3: The E2E Test Suite Deadlock

  * **Synopsis:** A multi-sprint effort to stabilize the E2E test suite was blocked by a series of cascading, complex failures.
  * **Root Cause:** The primary failure was that the test runner was made responsible for managing the application server's lifecycle, which proved to be fundamentally unstable. This was followed by a secondary failure where a test's own internal logic was flawed (it expected 5 tasks, but the app correctly only created 1), and the AI entered a rationalization loop instead of diagnosing the test itself.
  * **Solution:** The test runner and application server were permanently decoupled. The Architect intervened, invoking the "Circuit Breaker Protocol," to provide the correct diagnosis for the flawed test logic.
  * **Core Lessons:**
      * **Decouple Your Environment:** Do not make a test runner responsible for managing a long-lived server process.
      * **The Test is Also Code:** A failing test does not always mean the application is broken. The test script itself is a primary suspect for bugs.
      * **Uphold the Standard:** A sprint is not complete until all tests pass. Do not accept rationalizations for failure.

### Case Study #4: The "Sentry" False Positive

  * **Synopsis:** Following the newly instituted Sentry Protocol, a failing automated test for the "Delete Task" button led to a deep, multi-layered investigation.
  * **Root Cause:** After multiple failed fix attempts, a comprehensive diagnostic test proved that the application code was, in fact, correct. The root cause was a limitation in the Puppeteer test framework, where its synthetic `click()` event could not reliably trigger an inline `onclick` handler in the application's HTML, creating a convincing "false positive" bug report.
  * **Solution:** The investigation, guided by the Sentry Protocol, successfully protected the working application code from unnecessary changes. The failing test was removed, and "refactor `onclick` handlers to `addEventListener`" was added to the technical debt backlog.
  * **Core Lessons:**
      * **Be Aware of Test Framework Limitations:** A failing automated test can be a false positive.
      * **Always Verify with Manual Testing:** Before altering production code based on a failing test, a manual check is the final ground truth.
      * **The Sentry Protocol Works:** The protocol successfully prevented a "fix" from being applied to already-working code.

### Case Study #5: The 'False Confidence' Migration Crisis

  * **Synopsis:** A major sprint to migrate the application to React ("Operation: True Parity") was declared a success based on a suite of passing E2E tests. However, immediate manual testing revealed a catastrophic failure: the migrated application was visually and functionally broken, bearing little resemblance to the original prototype.
  * **Root Cause:** The automated tests, while technically passing, were not rigorous enough. They were "vanity tests" that used `page.goto()` to test components in isolation, failing to test the true, click-driven user navigation and overall page structure. This created a state of "false confidence" where the test suite was green, but the application was unusable.
  * **Solution:** The failed sprint was canceled. A new, more rigorous sprint ("Operation: Digital Twin") was initiated, governed by a "Hands-On Architecture" model where the Architect leads a meticulous, page-by-page audit before any remediation work is authorized.
  * **Core Lessons:**
      * Passing tests are meaningless if they don't accurately reflect the user's reality.
      * The Architect must be actively involved in the analysis of raw data, not just the approval of AI-generated plans.
      * A meticulous, page-by-page, element-by-element audit is required for any complex migration.

### Case Study #6: Operation True Parity - Migration Success

  * **Synopsis:** A major architectural migration from vanilla JavaScript to React/TypeScript was successfully completed using the Parity Protocol, achieving 100% functional parity across all three journey modules.
  * **Challenge:** Previous migration attempt ("False Confidence Migration Crisis") failed due to inadequate testing that created false confidence while the application was functionally broken.
  * **Solution Applied:** 
      * **Parity Protocol Implementation:** Used original working E2E tests as machine-readable specification
      * **Systematic Module-by-Module Approach:** Brainstorm → Choose → Objectives (5, 5, 7 steps respectively)
      * **Proven React Architecture Pattern:** JourneyStepPage component, useJourney hook, auto-save with 2-second debouncing
      * **Test-Driven Validation:** Each module achieved 100% parity before proceeding to next
  * **Quantified Results:**
      * **TP.3 Brainstorm Module:** ✅ 100% parity (5 steps) - Exit Code 0
      * **TP.4 Choose Module:** ✅ 100% parity (5 steps) - Exit Code 0  
      * **TP.5 Objectives Module:** ✅ 100% parity (7 steps) - Exit Code 0
      * **Navigation Audit:** ✅ Critical discrepancies identified and resolved via Sentry tests
  * **Core Lessons:**
      * **The Parity Protocol Works:** 100% success rate when applied systematically
      * **Test-Driven Migration is Reliable:** Original E2E tests provide definitive specification
      * **Modular Architecture Scales:** Proven patterns replicate consistently across modules
      * **Navigation Audits are Critical:** Sequential module flow must match original exactly

### Case Study #7: Navigation Discrepancy Resolution

  * **Synopsis:** Post-migration navigation audit revealed critical user flow regressions that were successfully identified and resolved using Sentry tests.
  * **Discovered Issues:**
      1. **Module Completion Flow Broken:** Brainstorm/Choose completion redirected to dashboard instead of next module's first step
      2. **Sidebar Home Link Context:** Required verification of context-aware behavior
  * **Sentry Protocol Application:**
      1. **REPORT:** Navigation discrepancies identified during manual testing
      2. **REPLICATE:** Created failing Sentry tests to prove bugs exist
      3. **REMEDIATE:** Implemented `getModuleCompletionUrl()` function with sequential navigation logic
      4. **RE-RUN:** Verified all 4 Sentry tests pass (Exit Code 0)
      5. **REGRESS:** Confirmed no impact on existing functionality
  * **Technical Implementation:**
      * **Sequential Navigation Logic:** Brainstorm → Choose Step 1, Choose → Objectives Step 1, Objectives → Dashboard
      * **Context-Aware Sidebar:** Verified existing implementation meets parity requirements
  * **Core Lessons:**
      * **Navigation Audits are Essential:** User flow regressions are common in migrations
      * **Sentry Tests Provide Proof:** Failing tests validate bug existence before fixes
      * **Sequential Module Flow is Critical:** Must match original vanilla JS behavior exactly

-----

## 7.0 Appendix B: Cascade Prompting Guide v1.0 (Official Addendum)

**Version:** 1.0
**Date:** July 18, 2025
**For:** Junior Architect working with Cascade AI
**Purpose:** Proven patterns for high-success AI collaboration

### Core Philosophy: The Five-Phase Protocol

Based on extensive testing, Cascade performs best with structured, phase-based prompts that separate concerns and provide clear verification steps.

#### **Phase 1: Intent & Plan**

Always start by stating your intent and asking Cascade to create a plan before implementation.

#### **Phase 2: Grounding (Read-Before-Write)**

**Critical:** Never let Cascade assume file contents. Always enforce the Read-Before-Write protocol.

#### **Phase 3: Implementation**

Keep implementation prompts focused on a single, specific change.

#### **Phase 4: Verification**

Always verify changes with explicit testing.

#### **Phase 5: Completion Check**

Confirm the objective was achieved.

### Critical Success Patterns

**1. Use "DECISION CONFIRMED:" Trigger**
This is Cascade's explicit approval mechanism.

**2. Separate Implementation from Operation**
Never combine code changes with test execution in the same prompt.

### Anti-Patterns to Avoid

  * **❌ Don't: Assume File Contents**
  * **❌ Don't: Combine Multiple Concerns**
  * **❌ Don't: Use Vague Approval Language**

### Emergency Protocols

**If Cascade Gets Stuck (Circuit Breaker Protocol):**

```
STOP. Circuit Breaker Protocol activated.

The current approach is not working. Let's reset:
1. What is the exact current state?
2. What is the simplest possible fix?
3. What is the single next action to take?
```

**If Tests Keep Failing (Diagnostic Protocol):**

```
The tests are still failing. Let's diagnose systematically:
1. Read the exact error message.
2. Identify which specific assertion is failing.
3. Determine if this is a test logic error or application error.
```
